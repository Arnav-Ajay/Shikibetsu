{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loading_and_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGHRAKkwLum"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLSCsv0FTg2"
      },
      "source": [
        "# change to your dataset path\n",
        "home_dir = '/content/drive/MyDrive/Shikibetsu_Datasets/prepared/'\n",
        "data_dir = os.path.join(home_dir, 'Market-1501/')\n",
        "\n",
        "USE_VALIDATION_DATA = False\n",
        "num_workers = 1 #parallel or not\n",
        "batch_size = 16"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BW4H645mFIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599945ba-e0e3-46b3-d830-a4b3b07f96bf"
      },
      "source": [
        "transform_train_list = [\n",
        "        transforms.Resize((288, 144), interpolation=3),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "\n",
        "transform_val_list = [\n",
        "        transforms.Resize(size=(288,144),interpolation=3), #Image.BICUBIC\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose( transform_train_list ),\n",
        "    'val': transforms.Compose(transform_val_list),\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHxlq7gCHhNO"
      },
      "source": [
        "train_all = ''\n",
        "if not USE_VALIDATION_DATA:\n",
        "     train_all = '_all'\n",
        "\n",
        "image_datasets = {}\n",
        "image_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n",
        "                                          data_transforms['train'])\n",
        "image_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl0dkgS0HWgO"
      },
      "source": [
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=num_workers)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "inputs, classes = next(iter(dataloaders['train']))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpGEuTt6Hzaa",
        "outputId": "01f09fc8-b318-4f55-dfa6-c51210f6ee76"
      },
      "source": [
        "print(dataset_sizes)\n",
        "print(classes)\n",
        "print(inputs[0].size())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 12185, 'val': 751}\n",
            "tensor([299, 147,  87, 493, 373, 345, 131, 412,  27, 636, 417, 353,  53, 293,\n",
            "        477, 377])\n",
            "torch.Size([3, 288, 144])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}